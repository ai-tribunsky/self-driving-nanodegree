# Behavioral Cloning Project

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

Overview
---
In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. 
The model trained with Keras will output a steering angle to an autonomous vehicle.

The Project
---
The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

### Dependencies

[CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit)

The simulator can be downloaded from the classroom. 
In the classroom, we have also provided sample data that you can optionally use to help train your model.

### Files Submitted & Code Quality

Project includes the following files:
* [train.py](train.py) containing the script to create and train the model
* [drive.py](drive.py) for driving the car in autonomous mode
* [model.h5](model.h5) containing a trained convolution neural network
* [video.mp4](video.mp4) containing video of model final predictions

### Model Architecture

Model architecture implemented in [train.py](train.py#L90) and based
on Nvidia network described in [End to End Learning for Self-Driving Cars](https://devblogs.nvidia.com/deep-learning-self-driving-cars/) paper.

| Layer         		| Output Shape 	 |     Description	        					          | 
|:---------------------:|:--------------:|:------------------------------------------------------ :|
| Cropping2D      		| (65, 320, 3)	 | Crops ROI of image                                      | 
| Lambda         		| (65, 320, 3)	 | Performs simple image normalization                     | 
| Convolution 2D   		| (31, 158, 24)	 | Filters=24; Kernel=5x5; Stride=2x2 with ReLU activation | 
| Convolution 2D   		| (14, 77, 36)	 | Filters=36; Kernel=5x5; Stride=2x2 with ReLU activation | 
| Convolution 2D   		| (5, 37, 48)	 | Filters=48; Kernel=5x5; Stride=2x2 with ReLU activation | 
| Convolution 2D   		| (3, 35, 64)	 | Filters=64; Kernel=3x3; Stride=1x1 with ReLU activation | 
| Convolution 2D   		| (1, 33, 64)	 | Filters=64; Kernel=3x3; Stride=1x1 with ReLU activation | 
| Flatten       		| (2112)		 |                                                         | 
| Dense          		| (1164)		 | With ReLU activation                                    | 
| Dense          		| (100)			 | With ReLU activation                                    | 
| Dense          		| (50) 			 | With ReLU activation                                    | 
| Dense          		| (1) 			 |                                                         | 

### Model Training Strategy

Dataset was split by 70/30 chunks for training and validation. 
Train and validation datasets contain serious amount of data which does not fit into GPU memory entirely.
That's why training was performed with small batches of images generated by generators.
Generator takes center, right and left image, performs steering angle adjust for left and right images.
```python
# Location: train.py:48

center_image_path = data_dir + '/' + batch_sample[0].strip()
left_image_path = data_dir + '/' + batch_sample[1].strip()
right_image_path = data_dir + '/' + batch_sample[2].strip()

center_angle = float(batch_sample[3])
left_angle = center_angle + angle_adjust
right_angle = center_angle - angle_adjust

images.extend([
    imageio.imread(center_image_path),
    imageio.imread(left_image_path),
    imageio.imread(right_image_path)
])
angles.extend([center_angle, left_angle, right_angle])
```

For mean square loss optimization RMSProp optimizer is used.
Model was trained with 5 epochs and batch size equal to 33 (11 center images, 11 left images, 11 right images)

### Model Evaluation

Model was evaluated on Track 1 and results can be seen in [video.mp4](video.mp4)